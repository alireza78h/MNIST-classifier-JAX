{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 18:10:11.859445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730326211.870911  110388 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730326211.874391  110388 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-30 18:10:11.885684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1730326213.540410  110388 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9685 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "#tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_dir = '/tmp/tfds'\n",
    "\n",
    "# as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
    "data, info = tfds.load(name=\"mnist\",\n",
    "                       data_dir=data_dir,\n",
    "                       as_supervised=True,\n",
    "                       with_info=True)\n",
    "\n",
    "data_train = data['train']\n",
    "data_test  = data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 28\n",
    "WIDTH  = 28\n",
    "CHANNELS = 1\n",
    "NUM_PIXELS = HEIGHT * WIDTH * CHANNELS\n",
    "NUM_LABELS = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from clu import metrics\n",
    "import flax\n",
    "import optax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, label):\n",
    "  \"\"\"Resize and preprocess images.\"\"\"\n",
    "  return (tf.cast(img, tf.float32)/255.0), label\n",
    "     \n",
    "data_train_vis = data_train.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfds.as_numpy(data_train.map(preprocess).batch(32).prefetch(1))\n",
    "test_data  = tfds.as_numpy(data_test.map(preprocess).batch(32).prefetch(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  \"\"\"A simple MLP model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(5,5))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2,2))\n",
    "    x = nn.Conv(features=64, kernel_size=(5,5))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2,2))\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = nn.Dense(features=1024)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'Conv_0': {'bias': (32,), 'kernel': (5, 5, 1, 32)},\n",
       "  'Conv_1': {'bias': (64,), 'kernel': (5, 5, 32, 64)},\n",
       "  'Dense_0': {'bias': (1024,), 'kernel': (43264, 1024)},\n",
       "  'Dense_1': {'bias': (10,), 'kernel': (1024, 10)}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "random_flattened_image = random.normal(key1, (1,28,28,1))\n",
    "params = model.init(key2, random_flattened_image) # Initialization call\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) # Checking output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.5565213 ,  1.2295414 , -0.245256  , -0.25400254,  1.2998863 ,\n",
       "        -0.5383994 , -0.08894709, -0.41814116, -0.9940413 ,  0.5447468 ]],      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, random_flattened_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flax.struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  accuracy: metrics.Accuracy\n",
    "  loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "  metrics: Metrics\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optax.sgd(learning_rate=0.01, momentum=0.9),\n",
    "    metrics=Metrics.empty())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(state, x, y):\n",
    "  logits = state.apply_fn(state.params, x)\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=y).mean()\n",
    "  metric_updates = state.metrics.single_from_model_output(\n",
    "    logits=logits, labels=y, loss=loss)\n",
    "  metrics = state.metrics.merge(metric_updates)\n",
    "  state = state.replace(metrics=metrics)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(train_state, x, y):\n",
    "  \"\"\"A single training step\"\"\"\n",
    "  def loss(params, images, targets):\n",
    "    \"\"\"Categorical cross entropy loss function.\"\"\"\n",
    "    logits = train_state.apply_fn(params, images)\n",
    "    loss_ce = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=targets).mean()\n",
    "    return loss_ce\n",
    "  loss_value, grads = jax.value_and_grad(loss)(train_state.params, x, y)\n",
    "  train_state = train_state.apply_gradients(grads=grads)\n",
    "  return train_state, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 18:11:16.696841: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (5, 5, 1, 32) but got shape (5, 5, 784, 32) instead for parameter \"kernel\" in \"/Conv_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m   y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mastype(jnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;66;03m#y = jax.nn.one_hot(y, NUM_LABELS)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m   state, loss_value \u001b[38;5;241m=\u001b[39m update(state, x, y)\n\u001b[1;32m     10\u001b[0m   state \u001b[38;5;241m=\u001b[39m compute_metrics(state, x, y)\n\u001b[1;32m     11\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(train_state, x, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m   loss_ce \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy_with_integer_labels(\n\u001b[1;32m      8\u001b[0m       logits\u001b[38;5;241m=\u001b[39mlogits, labels\u001b[38;5;241m=\u001b[39mtargets)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m loss_ce\n\u001b[0;32m---> 10\u001b[0m loss_value, grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss)(train_state\u001b[38;5;241m.\u001b[39mparams, x, y)\n\u001b[1;32m     11\u001b[0m train_state \u001b[38;5;241m=\u001b[39m train_state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_state, loss_value\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mupdate.<locals>.loss\u001b[0;34m(params, images, targets)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(params, images, targets):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Categorical cross entropy loss function.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m   logits \u001b[38;5;241m=\u001b[39m train_state\u001b[38;5;241m.\u001b[39mapply_fn(params, images)\n\u001b[1;32m      7\u001b[0m   loss_ce \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy_with_integer_labels(\n\u001b[1;32m      8\u001b[0m       logits\u001b[38;5;241m=\u001b[39mlogits, labels\u001b[38;5;241m=\u001b[39mtargets)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m loss_ce\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 6\u001b[0m   x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m))(x)\n\u001b[1;32m      7\u001b[0m   x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m      8\u001b[0m   x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmax_pool(x, window_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/mynenv/lib/python3.11/site-packages/flax/linen/linear.py:631\u001b[0m, in \u001b[0;36m_Conv.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m kernel_shape:\n\u001b[1;32m    626\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMask needs to have the same shape as weights. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShapes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    629\u001b[0m   )\n\u001b[0;32m--> 631\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[1;32m    632\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_init, kernel_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[1;32m    633\u001b[0m )\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m   kernel \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/mynenv/lib/python3.11/site-packages/flax/core/scope.py:989\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 989\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[1;32m    990\u001b[0m         name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[1;32m    991\u001b[0m       )\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (5, 5, 1, 32) but got shape (5, 5, 784, 32) instead for parameter \"kernel\" in \"/Conv_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for x, y in train_data:\n",
    "    y = y.astype(jnp.int32)\n",
    "    #y = jax.nn.one_hot(y, NUM_LABELS)\n",
    "    state, loss_value = update(state, x, y)\n",
    "    state = compute_metrics(state, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "\n",
    "  for metric,value in state.metrics.compute().items():\n",
    "    print(f\"Training set {metric} {value}\")\n",
    "  state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "  test_state = state\n",
    "  for x, y in test_data:\n",
    "    x = jnp.reshape(x, (len(x), NUM_PIXELS))\n",
    "    y = y.astype(jnp.int32)\n",
    "    test_state = compute_metrics(test_state, x, y)\n",
    "\n",
    "  for metric,value in test_state.metrics.compute().items():\n",
    "    print(f\"Test set {metric} {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
