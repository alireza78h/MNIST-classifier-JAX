{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from clu import metrics\n",
    "import flax\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jax.tree_util import tree_map\n",
    "from torch.utils import data\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "  def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "    super(self.__class__, self).__init__(dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        sampler=sampler,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=numpy_collate,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        timeout=timeout,\n",
    "        worker_init_fn=worker_init_fn)\n",
    "\n",
    "batch_size = 32\n",
    "n_targets = 10\n",
    "from jax.nn import one_hot\n",
    "class FlattenAndCast(object):\n",
    "  def __call__(self, pic):\n",
    "    return jnp.expand_dims(np.array(pic, dtype=jnp.float32) / 255, -1)\n",
    "mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
    "training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)\n",
    "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False, transform=FlattenAndCast())\n",
    "test_generator = NumpyLoader(mnist_dataset_test, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, x, training: bool = True):\n",
    "    x = nn.Conv(features=32, kernel_size=(5,5))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2,2))\n",
    "    x = nn.Conv(features=64, kernel_size=(5,5))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2,2))\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = nn.Dense(features=1024)(x)\n",
    "    x = nn.Dropout(rate=0.5, deterministic=not training)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "root_key = jax.random.key(seed=0)\n",
    "key1, key2, dropout_key = jax.random.split(key=root_key, num=3)\n",
    "random_flattened_image = random.normal(key1, (1,28,28,1))\n",
    "variables = model.init(key2, random_flattened_image, training=False) # Initialization call\n",
    "params = variables['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree_util.tree_map(lambda x: x.shape, params) # Checking output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply({'params': params}, random_flattened_image, training=True, rngs={'dropout': dropout_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flax.struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  accuracy: metrics.Accuracy\n",
    "  loss: metrics.Average.from_output('loss')\n",
    "class TrainState(train_state.TrainState):\n",
    "  metrics: Metrics\n",
    "  key: jax.Array\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    key=dropout_key,\n",
    "    tx=optax.sgd(learning_rate=0.01, momentum=0.9),\n",
    "    metrics=Metrics.empty())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(state, x, y):\n",
    "  logits = state.apply_fn(\n",
    "      {'params': state.params},\n",
    "      x,\n",
    "      training=False\n",
    "      )\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=y).mean()\n",
    "  metric_updates = state.metrics.single_from_model_output(\n",
    "    logits=logits, labels=y, loss=loss)\n",
    "  metrics = state.metrics.merge(metric_updates)\n",
    "  state = state.replace(metrics=metrics)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(train_state, x, y, dropout_key):\n",
    "  dropout_train_key = jax.random.fold_in(key=dropout_key, data=train_state.step)\n",
    "  def loss(params, images, targets):\n",
    "    logits = train_state.apply_fn(\n",
    "      {'params': params},\n",
    "      images,\n",
    "      training=True,\n",
    "      rngs={'dropout': dropout_train_key}\n",
    "      )\n",
    "    loss_ce = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=targets).mean()\n",
    "    return loss_ce\n",
    "  loss_value, grads = jax.value_and_grad(loss)(train_state.params, x, y)\n",
    "  train_state = train_state.apply_gradients(grads=grads)\n",
    "  return train_state, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for x, y in training_generator:\n",
    "    y = y.astype(jnp.int32)\n",
    "    state, loss_value = update(state, x, y, dropout_key)\n",
    "    state = compute_metrics(state, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "\n",
    "  for metric,value in state.metrics.compute().items():\n",
    "    print(f\"Training set {metric} {value}\")\n",
    "  state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "  test_state = state\n",
    "  for x, y in test_generator:\n",
    "    y = y.astype(jnp.int32)\n",
    "    test_state = compute_metrics(test_state, x, y)\n",
    "\n",
    "  for metric,value in test_state.metrics.compute().items():\n",
    "    print(f\"Test set {metric} {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
